openstack-cluster:
  addons:
    monitoring:
      enabled: true
      lokiStack:
        enabled: false
      kubePrometheusStack:
        release:
          values:

            defaultRules:
              additionalRuleLabels:
                # these values used for alerting only - part of the email subject tag-line
                cluster: not-set
                env: dev
            
            alertmanager:
              enabled: false
              service:
                type: ClusterIP
              
              ingress:
                annotations:
                  cert-manager.io/cluster-issuer: "self-signed"
                
                ingressClassName: nginx
                enabled: true
                hosts: 
                  -  alertmanager.example.com
                paths:
                  - "/"

                ingressPerReplica:
                  enabled: false

              config:
                global:
                  # how long to wait until a resolved message comes in - if an alert has been sent and was subsequently resolved
                  resolve_timeout: 1h
                  smtp_smarthost: 
                  smtp_from:
                  smtp_require_tls: false

                route:
                  receiver: 'null'
                  routes:
                    - receiver: 'null'
                      matchers:
                      - alertname = "Watchdog"
                    - receiver: default-receiver
                      # how long the alert needs to be firing before we send an alert
                      # this is high so more likely that issues k8s resolves by itself will be ignored
                      group_wait: 30m

                      # how long to wait before sending another alert 
                      # - which ALSO contains any new alerts belonging to the same group (if they get triggered after the initial alert is sent)
                      # - we don't want to clog up the ticket queue with the same alert - so just send the same ticket again after our SLA expires
                      group_interval: 2d

                      # how long to wait before sending the same alert if not fixed
                      # - we don't want to clog up the ticket queue with the same alert - so just send the same ticket again after our SLA expires
                      # different from group_interval - because it should contain the same info in the email
                      repeat_interval: 2d
                      group_by: ['cluster', 'service']
                      matchers:
                      - severity=~"critical|warning"

                inhibit_rules:
                  - source_matchers: [severity="critical"]
                    target_matchers: [severity="warning"]
                    equal: [alertname, cluster, service]

                  - source_matchers:
                      - 'severity = warning'
                    target_matchers:
                      - 'severity = info'
                    equal: [alertname, cluster, service]

                  - source_matchers:
                      - 'alertname = InfoInhibitor'
                    target_matchers:
                      - 'severity = info'
                    equal: ['namespace']

                  - target_matchers:
                      - 'alertname = InfoInhibitor'

                receivers:
                - name: 'null'
                - name: default-receiver
                  email_configs:
                  - to:
                    send_resolved: true
                    headers:
                      subject: |
                        {%- raw %}
                        "({{ .CommonLabels.env }} : {{ .CommonLabels.cluster }}) {{ .Status | toUpper }} {{ .CommonLabels.alertname }}"
                        {%- endraw %}
                    html: |-
                      {%- raw %}
                      <h3>You have the following alerts:</h3>
                      {{ range .Alerts }}
                      <p><b>{{.Labels.alertname}}</b>
                        <ul>{{ range .Annotations.SortedPairs }}
                        <li>{{ .Name }} = {{ .Value }}</li>
                        {{ end }}</ul>
                        <ul>{{ range .Labels.SortedPairs }}
                        <li>{{ .Name }} = {{ .Value }}</li>
                        {{ end }}</ul>
                        {{ .GeneratorURL }}</p>
                      {{ end }}
                      {%- endraw %}
                    text: |-
                      {%- raw %}
                      You have the following alerts:
                      {{ range .Alerts }}
                      * {{.Labels.alertname}}
                        {{ range .Annotations.SortedPairs }}
                        {{ .Name }} = {{ .Value }}
                        {{ end }}
                        {{ range .Labels.SortedPairs }}
                        {{ .Name }} = {{ .Value }}
                        {{ end }}
                        {{ .GeneratorURL }}
                      {{ end }}
                      {%- endraw %}

                templates:
                  - '/etc/alertmanager/config/*.tmpl'

              alertmanagerSpec:
                # turn off persistent storage so cinder volume doesn't get created
                # TODO: remove this when cinder creation issue resolved          
                storage: 
                  emptyDir: 
                    medium: Memory


            grafana:
              enabled: true
              
              service:
                type: ClusterIP
              
              ingress:
                annotations:
                  cert-manager.io/cluster-issuer: "self-signed"
                
                ingressClassName: nginx
                enabled: true
                path: /
                hosts: 
                  -  grafana.example.com

            prometheus-operator:
              enabled: true

            prometheus:
              enabled: true

              service:
                type: ClusterIP

              ingress:
                annotations:
                  cert-manager.io/cluster-issuer: "self-signed"
                ingressClassName: nginx
                enabled: true
                paths:
                  - /
                hosts: 
                  -  prometheus.example.com
              
              prometheusSpec:
                # turn off persistent storage so cinder volume doesn't get created
                # need to debug cinder volume creation issue
                storageSpec: 
                  emptyDir: 
                    medium: Memory

            # TODO look into Thanos for multi-cluster monitoring
            # thanos service discovery on sidecar
            thanosService:
              enabled: false
                          
            # thanos scrape metrics on sidecar
            thanosServiceMonitor:
              enabled: false

            # thanos access on sidecar
            thanosServiceExternal:
              enabled: false