# Storage Setup

There are several apps that enable storage for your cluster. These currently include:
1. cinder (out-of-the-box)
2. longhorn
3. manila

# Cinder

We utilise the Cinder CSI (Container Storage Interface) Driver to manage the lifecycle of OpenStack Cinder Volumes. See [Cinder CSI Helm Chart](https://github.com/kubernetes/cloud-provider-openstack/tree/master/charts/cinder-csi-plugin)

No steps are required to enable cinder storage, this is available out-of-the-box when you create a cluster using CAPI - with this repo or [Cloud Capi Values](https://github.com/stfc/cloud-capi-values/tree/master)

# Pre-requisites

Make sure that the project your cluster is built on has spare volume instances and volume storage capacity - if not, raise a ticket to cloud-support.stfc.ac.uk 

# Longhorn

Longhorn is a Cloud Native application for presistent block storage.  See [Longhorn docs](https://longhorn.io/docs/latest/). It utilises the local storage available on worker nodes, creating and managing replicas of container volumes to keep data persistent 

To deploy Longhorn we utilise the longhorn helm chart. See [Chart Repo](https://github.com/longhorn/longhorn/tree/master/chart).

## Pre-deployment steps

#### 1. **(Optional)** Label nodes to run longhorn on

Make sure you have labelled your nodes so that longhorn can use them as storage nodes. 

If you're using our `capi-infra` chart to manage your CAPI cluster - these are set for you so you don't need to do anything.  

If not, you want to label your worker nodes, the default label is - `longhorn.store.nodeselect/longhorn-storage-node: "true"`. Run the following on all your worker nodes:

```bash
kubectl label node my-worker-node longhorn.store.nodeselect/longhorn-storage-node="true" -n clusters
```

If you want to change the label you can change this in the cluster-specific values like so. Not recommended unless you know what you are doing

```yaml
longhorn:	
  longhornManager:	
    nodeSelector: 	
      # change this to whatever label you want	
      longhorn.store.nodeselect/longhorn-storage-node: true	
```	

#### 2. **(Optional)** Add TLS secret 	

Longhorn is configured to use TLS by default - it is setup to create a self-signed certificate using cert-manager.

> [!CAUTION]
> For Production Systems, use a production certificate.
> Do **NOT** use a self-signed certificate.


To enable letsencrypt certificate, you must enable cert-manager on your cluster - see [cert-manager docs](./misc.md) and add this to your values file
```yaml

longhorn:
  ingress:
    annotation:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
```
 
## Deployment

You can deploy the chart as standalone

```bash
cd cloud-deployed-apps/charts/prod/longhorn # or equivalent folder for dev/staging
helm dependency upgrade .
helm install longhorn . -n galaxy --create-namespace
```

or you can use argocd to install it - see [Deploying Apps](../deploying-apps.md)

## Common Problems 

### 1. Longhorn web UI is giving a 500 and ArgoCD is stuck processing

Doing `kubectl get ds -n longhorn-system` shows a deployment with 0/0 instances

**Solution**:  You may be missing annotations on your node, refer to pre-deployment step 1

# Manila

> [!CAUTION]
> Needs testing

Manila is a service that provides shared filesystem for services on STFC Cloud. The Manila CSI enables kubernetes services to provision and manage Manila shares. See [Manila CSI Helm Chart](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/manila-csi-plugin/using-manila-csi-plugin.md).

See our Docs on [using Manila on Kubernetes](https://stfc.atlassian.net/wiki/spaces/SC/pages/117375031/Manila+on+Kubernetes) 

## Prerequisites

Make sure that the project your cluster is built on is permitted to create shares and has spare share instances and share storage capacity - if not, raise a ticket to cloud-support.stfc.ac.uk 

 ## Deployment

You can deploy the chart as standalone

```bash
cd cloud-deployed-apps/charts/prod/longhorn # or equivalent folder for dev/staging
helm dependency upgrade .
helm install longhorn . -n galaxy --create-namespace
```


### Argocd Deployment

or you can use argocd to install it - see [Deploying Apps](../deploying-apps.md)

>[!WARNING] 
> you need to tell argocd to ignore some clusterroles because they get updated after deployment and causes out-of-sync issues
> add the following config to the `<cluster-name>-apps` ApplicationSet in `apps.yaml` for your cluster
> ```yaml
> spec:
>  templatePatch: |
>    {{- if eq .name "manila-csi" }}
>      spec:
>        ignoreDifferences:
>          - group: rbac.authorization.k8s.io
>            kind: ClusterRole
>            name: manila-csi-openstack-manila-csi-controllerplugin
>            jsonPointers:
>              - /rules
>          - group: rbac.authorization.k8s.io
>            kind: ClusterRole
>            name: manila-csi-openstack-manila-csi-nodeplugin
>            jsonPointers:
>              - /rules
>    {{- end }}
> ```

## Post-deployment steps

Once you deploy manila-csi onto your cluster - it wont work until you give it appropriate credentials to access openstack

### 1. Create an application credential for manila-csi 

> [!NOTE]
> You should reuse the application credential you used to create the cluster 

### 2. Create and deploy a secret onto your cluster

using your application credential - create the secret 

> [!NOTE]
> TODO: We should do this automatically

```bash
kubectl create secret generic csi-manila-secret --from-literal=os-projectID=<project-id>  --from-literal=os-userName=<username>  --from-literal=os-password=<password>  --from-literal=os-domainName=<domain> -n manila-csi
```
